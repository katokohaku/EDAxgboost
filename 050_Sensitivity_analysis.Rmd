---
author: "Satoshi Kato"
title: rule extraction from xgboost model"
date: "`r format(Sys.time(), '%Y/%m/%d')`"
output:
  html_document:
    fig_caption: yes
    pandoc_args:
      - --from
      - markdown+autolink_bare_uris+tex_math_single_backslash-implicit_figures
    toc: yes
    keep_md: yes
  word_document:
    toc: yes
    toc_depth: 3
  pdf_document:
    toc: yes
    toc_depth: 3
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_knit$set(progress = TRUE, 
                     verbose  = TRUE, 
                     root.dir = ".")

knitr::opts_chunk$set(collapse = FALSE, 
                      comment = "", 
                      message = TRUE, 
                      warning = FALSE, 
                      include = TRUE,
                      echo    = TRUE)

set.seed(1)
```

```{r install.requirements, eval = FALSE}
install.packages("DiagrammeR", dependencies = TRUE)
install.packages("inTtrees", dependencies = TRUE)

install.packages("devtools", dependencies = TRUE)
devtools::install_github("AppliedDataSciencePartners/xgboostExplainer")

```

```{r require.packages, message=FALSE}
require(tidyverse)
require(xgboost)
require(AUC)
require(caret)
require(DALEX)

```

# Preparation (continued)

```{r load.model.and.data}
loaded.obs  <- readRDS("./middle/data_and_model.Rds")

model.xgb   <- loaded.obs$model$xgb 

train.label <- loaded.obs$data$train$label
train.matrix <- loaded.obs$data$train$matrix
train.xgb.DMatrix <- xgb.DMatrix("./middle/train.xgbDMatrix")

test.label  <- loaded.obs$data$test$label
test.matrix <- loaded.obs$data$test$matrix
test.xgb.DMatrix  <- xgb.DMatrix("./middle/test.xgbDMatrix")

```
# Marginal Response for a Single Variable

```{r explainer.DALEX}
explainer.xgb <- DALEX::explain(model.xgb, 
                                data  = test.matrix, 
                                y     = test.label, 
                                label = "xgboost")

var.imp <- xgb.importance(model = model.xgb,
                          feature_names = dimnames(train.xgb.DMatrix)[[2]])

var.imp %>% mutate_if(is.numeric, round, digits = 4)
target.feature <- var.imp$Feature %>% head(5)
target.feature
```

## Partial Dependence Plots (PDP)

```{r}
plot.pdps <- list()
for(feature.name in target.feature){
  pdp <- variable_response(explainer.xgb,
                           variable =  feature.name,
                           type = "pdp")
  plot.pdps[[feature.name]] <- plot(pdp) + ggtitle(feature.name)
}
plot.pdps[[1]] 

```

## Accumulated Local Effects Plots (ALE Plot)

```{r}
plot.ales <- list()
for(feature.name in target.feature){
  ale <- variable_response(explainer.xgb,
                           variable =  feature.name,
                           type = "ale")
  plot.ales[[feature.name]] <- plot(ale) + ggtitle(feature.name)
}
plot.ales[[1]] 

```



```{r}
for(feature.name in target.feature){
gridExtra::grid.arrange(plot.pdps[[feature.name]], 
                        plot.ales[[feature.name]], ncol=2)
}
```


# SHAP contribution dependency plots

Visualizing the SHAP feature contribution to prediction dependencies on feature value.

These scatterplots represent how SHAP feature contributions depend of feature values. The similarity to partial dependency plots is that they also give an idea for how feature values affect predictions. However, in partial dependency plots, we usually see marginal dependencies of model prediction on feature value, while SHAP contribution dependency plots display the estimated contributions of a feature to model prediction for each individual case.

When plot_loess = TRUE is set, feature values are rounded to 3 significant digits and weighted LOESS is computed and plotted, where weights are the numbers of data points at each rounded value.

Note: SHAP contributions are shown on the scale of model margin. E.g., for a logistic binomial objective, the margin is prediction before a sigmoidal transform into probability-like values. Also, since SHAP stands for "SHapley Additive exPlanation" (model prediction = sum of SHAP contributions for all features + bias), depending on the objective used, transforming SHAP contributions for a feature from the marginal to the prediction space is not necessarily a meaningful thing to do.

```{r, fig.height=8, fig.width=8}
xgb.plot.shap(data  = train.matrix,
              model = model.xgb, 
              # trees = trees0, 
              # target_class = 1, 
              # plot_loess = FALSE,
              top_n = 6,
              n_col = 2, col = col, pch = 16, pch_NA = 17)

```

